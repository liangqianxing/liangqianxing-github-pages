---
title: 线性代数基础
date: 2025-11-13 01:30:28
categories:
  - 数学
tags:
  - 线性代数
  - 矩阵
mathjax: true
toc:
  enable: true
  number: false
---

线性代数是机器学习、图形学、控制论乃至量子计算的底层语言。学习过程中如果只背公式，往往无法把抽象符号与具体场景联系起来。本文从零开始梳理关键概念、常见例子与学习路线，帮助你在“理解—计算—应用”之间建立桥梁。

## 1. 为什么线性代数如此重要

- 数据表示：向量可以描述单个样本的特征，矩阵可以并行操作一批样本。
- 线性变换：模型权重（全连接层、卷积核）本质都是线性映射。
- 优化与分解：梯度、Hessian、奇异值分解 (SVD) 等都植根于线性代数。

## 2. 前置知识清单

1. **代数基础**：熟悉实数运算、因式分解、函数图像。
2. **集合与映射**：理解“输入 $\rightarrow$ 输出”的函数关系，知道多元函数的含义。
3. **基础几何**：二维平面、三维空间中的点、向量、角度与面积。
4. **初等数列**：能处理求和符号 $\sum_{i=1}^{n} a_i$ 与简单递推。

若上述内容尚不牢固，建议先配合高中代数/几何教材或可汗学院的基础课程复习。

## 3. 向量：既是箭头也是列表

- **定义**：$n$ 维向量写作 $\mathbf{x} = (x_1, x_2, \dots, x_n)^\top$，通常当作列向量。
- **几何直觉**：二维向量是平面上的箭头，长度 $\|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2}$ 表示箭头的“强度”，方向由坐标确定。
- **现实例子**：影评情感向量 $\mathbf{x} = (0.8, -0.2, 0.1)$ 可表示“正面、负面、中性”特征贡献。
- **基本运算**：
  - 加法：$\mathbf{x} + \mathbf{y}$ 等价于把两个位移连在一起。
  - 标量乘法：$a\mathbf{x}$ 拉伸或压缩向量长度。
  - 点积：$\mathbf{x}^\top \mathbf{y} = \sum_i x_i y_i = \|\mathbf{x}\|\|\mathbf{y}\|\cos\theta$，衡量相似度或投影。

**示例**：设用户喜好向量 $\mathbf{p} = (3, 1)$ 表示“动作片”“爱情片”的偏好权重，电影 A 的向量为 $\mathbf{a} = (2, 1)$，电影 B 的向量为 $\mathbf{b} = (0, 2)$。点积
$$
\mathbf{p}^\top \mathbf{a} = 3 \times 2 + 1 \times 1 = 7,\qquad
\mathbf{p}^\top \mathbf{b} = 3 \times 0 + 1 \times 2 = 2
$$
说明用户与电影 A 的特征更接近，推荐系统就会优先推送电影 A。

## 4. 向量空间、线性组合与基

- **向量空间**：在加法与标量乘法下封闭的集合。例如所有二维向量构成 $\mathbb{R}^2$。
- **线性组合**：$\mathbf{v} = \alpha \mathbf{u}_1 + \beta \mathbf{u}_2$。若若干向量的线性组合能覆盖整个空间，它们就是该空间的生成集。
- **基与维度**：最小生成集叫基 (basis)。二维空间常用标准基 $\mathbf{e}_1=(1,0)^\top$、$\mathbf{e}_2=(0,1)^\top$。基向量数量即维度。
- **形象例子**：若以“甜味”“酸味”两个向量表示饮品口味空间，任何饮品都能写成它们的线性组合。换基就像改用“柑橘味”“莓果味”描述同一空间。

## 5. 矩阵：线性变换的载体

- **定义**：$m \times n$ 矩阵 $\mathbf{A}$ 是 $m$ 行 $n$ 列的数表，可看作把 $n$ 维输入映射到 $m$ 维输出的线性函数。
- **矩阵作用**：$\mathbf{y} = \mathbf{A}\mathbf{x}$。矩阵的列向量描述基向量被变换后的结果。
- **几何例子**：
  - 缩放矩阵 $\begin{bmatrix}2 & 0 \\ 0 & 0.5\end{bmatrix}$：水平方向放大 2 倍、竖直方向压缩为原来的一半。
  - 旋转矩阵 $\begin{bmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{bmatrix}$：围绕原点逆时针旋转 $\theta$。
- **组合变换**：两个变换先后执行等价于矩阵连乘 $\mathbf{C}\mathbf{B}\mathbf{A}\mathbf{x}$。

可以把矩阵视作“坐标轴变形机器”。二维标准坐标系的基为
$$
\mathcal{B}_{\text{std}} = \left\{
\mathbf{e}_1 =
\begin{bmatrix}
1 \\ 0
\end{bmatrix},
\quad
\mathbf{e}_2 =
\begin{bmatrix}
0 \\ 1
\end{bmatrix}
\right\}.
$$
矩阵 $\mathbf{A}$ 会把 $\mathbf{e}_1$ 映射到 $\mathbf{A}\mathbf{e}_1$，把 $\mathbf{e}_2$ 映射到 $\mathbf{A}\mathbf{e}_2$。这两个新向量就是“变形后”的坐标轴。任意向量 $\mathbf{x} = (x_1, x_2)^\top$ 可以写成 $x_1 \mathbf{e}_1 + x_2 \mathbf{e}_2$ ，因此

$$
\mathbf{A}\mathbf{x}
=
\mathbf{A}(x_1 \mathbf{e}_1 + x_2 \mathbf{e}_2)
=
x_1 (\mathbf{A}\mathbf{e}_1) + x_2 (\mathbf{A}\mathbf{e}_2).
$$

这条公式告诉我们：**矩阵的列向量就是被拉伸或扭曲后的坐标轴**，原向量的系数 $x_1,x_2$ 沿着新轴相加，就得到最终坐标。



**坐标轴表达示例**：设

$$
\mathbf{A} =
\begin{bmatrix}
1 & 1 \\
0 & 1
\end{bmatrix}
$$

（水平切变矩阵）。有
$$
\mathbf{A}\mathbf{e}_1 =
\begin{bmatrix}
1 \\ 0
\end{bmatrix},
\qquad
\mathbf{A}\mathbf{e}_2 =
\begin{bmatrix}
1 \\ 1
\end{bmatrix}.
$$
因此 $x$ 轴保持不变，而 $y$ 轴被倾斜到方向 $(1,1)^\top$。任意点
$$
\mathbf{x} =
\begin{bmatrix}
2 \\ 1
\end{bmatrix}
$$
会被映射为

$$
\mathbf{A}\mathbf{x}
= 2\mathbf{A}\mathbf{e}_1 + 1\mathbf{A}\mathbf{e}_2
= (2,0)^\top + (1,1)^\top
= (3,1)^\top
$$

，可以直观看出“沿新 $y$ 轴被拖拽”这一效果。

**示例**：图像中的像素点 $\mathbf{x} = (1, 2)^\top$ 先进行 45° 旋转，再做 2 倍放大。对应矩阵
$$
\mathbf{R} =
\begin{bmatrix}
\frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\
\frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2}
\end{bmatrix},
\qquad
\mathbf{S} =
\begin{bmatrix}
2 & 0 \\
0 & 2
\end{bmatrix}.
$$
最终坐标 $\mathbf{y} = \mathbf{S}\mathbf{R}\mathbf{x} = (-\sqrt{2},\ 3\sqrt{2})^\top$，可直观看到“先旋转再缩放”的顺序性。

## 6. 矩阵基本运算

| 运算 | 表达式 | 含义 |
| ---- | ---- | ---- |
| 加法 | $\mathbf{A} + \mathbf{B}$ | 对应元素相加 |
| 数乘 | $c\mathbf{A}$ | 所有元素乘以常数 $c$ |
| 乘法 | $\mathbf{C} = \mathbf{A}\mathbf{B}$，其中 $c_{ij} = \sum_k a_{ik}b_{kj}$ | 复合变换或把“特征 $\rightarrow$ 输出”连接起来 |
| 转置 | $\mathbf{A}^\top$ | 交换行列，点积可写为 $\mathbf{x}^\top\mathbf{y}$ |
| 逆矩阵 | $\mathbf{A}^{-1}$，满足 $\mathbf{A}\mathbf{A}^{-1} = \mathbf{I}$ | 撤销变换，只有可逆矩阵才存在 |

理解矩阵乘法的“行 × 列”视角十分关键：每一行代表一个输出变量如何聚合输入各分量。

## 7. 线性方程组与高斯消元

把方程组写成 $\mathbf{A}\mathbf{x} = \mathbf{b}$ 能统一讨论求解流程：

$$
\begin{cases}
2x + y = 5 \\
x - y = 1
\end{cases}
\quad\Rightarrow\quad
\begin{bmatrix}
2 & 1 \\
1 & -1
\end{bmatrix}
\begin{bmatrix}
x \\ y
\end{bmatrix}
=
\begin{bmatrix}
5 \\ 1
\end{bmatrix}.
$$

通过**高斯消元**（行变换）可化为上三角矩阵，再回代得到解。几何上，该例表示两条直线的交点；若系数矩阵行向量共线则无唯一交点。

**示例 1（唯一解）**：上式增广矩阵
$$
\begin{bmatrix}
2 & 1 & | & 5 \\
1 & -1 & | & 1
\end{bmatrix}
\xrightarrow{R_1 \leftarrow R_1 - 2R_2}
\begin{bmatrix}
0 & 3 & | & 3 \\
1 & -1 & | & 1
\end{bmatrix}
$$
得到 $y = 1$、$x = 2$。二者对应平面直线在点 $(2,1)$ 相交。

**示例 2（欠定/最小二乘）**：若

$$
\mathbf{A} =
\begin{bmatrix}
1 & 1 \\
1 & 1
\end{bmatrix},
\mathbf{b} =
\begin{bmatrix}
2 \\
3
\end{bmatrix}
$$

，
两行相同导致方程组无解。最小二乘解

$$
\hat{\mathbf{x}} = (\mathbf{A}^\top \mathbf{A})^{-1}\mathbf{A}^\top \mathbf{b} = (2.5,\ 2.5)^\top
$$

表示找到距离两条“重合直线”最近的点。

## 8. 行列式：体积与可逆性的度量

- 行列式 $\det(\mathbf{A})$ 衡量单位立方体经过 $\mathbf{A}$ 变换后的体积缩放。
- 若 $\det(\mathbf{A}) = 0$，说明变换把空间压成更低维子空间，矩阵不可逆。
- 二维示例：矩阵 $\begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}$ 的行列式为 $1\cdot4 - 2\cdot3 = -2$，表示面积缩放 2 倍并翻转方向。

**三维示例**：设

$$
\mathbf{A} =
\begin{bmatrix}
1 & 0 & 1 \\
2 & 1 & 0 \\
0 & 1 & 1
\end{bmatrix}
$$

，
利用展开可得
$$
\det(\mathbf{A}) = 1\cdot(1\cdot1-0\cdot1) - 0 + 1\cdot(2\cdot1-1\cdot0) = 
3.$$


这意味着单位立方体被拉伸为体积为 3 的平行六面体，并保持右手坐标系方向。

## 9. 特征值与特征向量

特征向量满足 $\mathbf{A}\mathbf{v} = \lambda \mathbf{v}$，即变换后方向不变，仅被放大或缩小。

- **现实场景**：主成分分析 (PCA) 中，协方差矩阵的特征向量就是最大方差方向，特征值表示该方向的方差大小。
- **计算方式**：解特征方程 $\det(\mathbf{A}-\lambda\mathbf{I}) = 0$ 得到特征值，再回代求特征向量。
- **几何意义**：找到“不会被扭曲方向”的轴，便于判断系统的稳定性或数据的主方向。

## 10. 正交性、投影与最小二乘

- **正交向量**：$\mathbf{u}^\top \mathbf{v} = 0$，夹角为 $90^\circ$。
- **正交矩阵**：$\mathbf{Q}^\top \mathbf{Q} = \mathbf{I}$，既保持长度又保持角度。
- **投影公式**：
  $$
  \text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\mathbf{u}^\top \mathbf{v}}{\mathbf{u}^\top \mathbf{u}} \mathbf{u}.
  $$
- **最小二乘**：当 $\mathbf{A}\mathbf{x} = \mathbf{b}$ 无精确解时，选择
  $$
  \hat{\mathbf{x}} = (\mathbf{A}^\top \mathbf{A})^{-1}\mathbf{A}^\top \mathbf{b},
  $$
  使残差 $\|\mathbf{A}\mathbf{x} - \mathbf{b}\|_2$ 最小，相当于把 $\mathbf{b}$ 投影到矩阵列空间上。

**投影示例**：令 $\mathbf{u} = (1, 1, 0)^\top$ 表示“沿对角线移动”的方向，$\mathbf{v} = (2, 1, 3)^\top$ 表示某个三维信号。将 $\mathbf{v}$ 投影到 $\mathbf{u}$ 上得到
$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{(2,1,3)\cdot(1,1,0)}{1^2+1^2} (1,1,0)^\top = \frac{3}{2} (1,1,0)^\top
$$
意味着信号在“对角线平面”上的分量是 $(1.5, 1.5, 0)^\top$，剩余的 $(0.5, -0.5, 3)^\top$ 则是与 $\mathbf{u}$ 正交的噪声。

## 11. 常见矩阵分解

| 分解 | 形式 | 典型用途 |
| ---- | ---- | ---- |
| LU | $\mathbf{A} = \mathbf{L}\mathbf{U}$，$\mathbf{L}$ 为下三角，$\mathbf{U}$ 为上三角 | 一次分解，多次求解 $\mathbf{A}\mathbf{x}=\mathbf{b}$ |
| QR | $\mathbf{A} = \mathbf{Q}\mathbf{R}$，$\mathbf{Q}$ 正交，$\mathbf{R}$ 上三角 | 正交化、稳定地解最小二乘 |
| 特征分解 | $\mathbf{A} = \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1}$（对称矩阵可正交对角化） | 谱聚类、PCA、动力系统分析 |
| SVD | $\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top$ | 降维、伪逆、低秩近似、推荐系统 |

分解的本质是把复杂映射拆解成“旋转 + 缩放 + 投影”等易理解的步骤。

### 奇异值分解 (SVD) 的 LaTeX 图示

SVD 把任意矩阵拆成三段：

1. $\mathbf{V}^\top$：在输入空间旋转/翻转，使数据对齐到“主轴”方向。
2. $\mathbf{\Sigma}$：保留非负奇异值 $\sigma_1 \ge \sigma_2 \ge \cdots$，沿坐标轴做纯拉伸或压缩。
3. $\mathbf{U}$：把变形后的结果放回输出空间，再旋转/翻转到目标坐标系。

为了兼容 Hexo 默认的 MathJax 配置，我们可以用一条“箭头链”表示 SVD 的三步变换：

$$
\begin{aligned}
(\cos\theta,\ \sin\theta)^\top
&\xrightarrow{\ \mathbf{V}^\top\ }
\mathbf{V}^\top(\cos\theta,\ \sin\theta)^\top
\xrightarrow{\ \mathbf{\Sigma}\ }
\mathbf{\Sigma}\mathbf{V}^\top(\cos\theta,\ \sin\theta)^\top \\
&\xrightarrow{\ \mathbf{U}\ }
\mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top(\cos\theta,\ \sin\theta)^\top.
\end{aligned}
$$

这条箭头链展示了“旋转 → 缩放 → 再旋转”的流水线：左侧单位圆上的每个点被 $\mathbf{V}^\top$ 对齐后，再由 $\mathbf{\Sigma}$ 拉伸成椭圆，最终通过 $\mathbf{U}$ 映射到输出空间。最大的奇异值 $\sigma_1$ 表示矩阵能把某个方向放大的最大倍数，越大说明该方向携带的能量越多；奇异值快速衰减意味着矩阵近似低秩。

**数值示例**：对于
$$
\mathbf{A} =
\begin{bmatrix}
3 & 1 \\
0 & 2
\end{bmatrix},
$$
有
$$
\mathbf{U} \approx
\begin{bmatrix}
0.957 & -0.290 \\
0.290 & \phantom{-}0.957
\end{bmatrix},
\quad
\mathbf{\Sigma} \approx
\begin{bmatrix}
3.257 & 0 \\
0 & 1.842
\end{bmatrix},
\quad
\mathbf{V}^\top \approx
\begin{bmatrix}
0.882 & 0.472 \\
-0.472 & 0.882
\end{bmatrix}.
$$
在几何上：$\mathbf{V}^\top$ 把单位圆旋转约 $27^\circ$，$\mathbf{\Sigma}$ 将其拉成长轴 $3.257$、短轴 $1.842$ 的椭圆，最后 $\mathbf{U}$ 再旋转到输出坐标。用于降维时，只保留最大的奇异值与对应列即可获得最佳的低秩近似。

## 12. 线性代数与机器学习的衔接

1. **数据标准化**：零均值处理等价于把样本投影到均值向量的正交补。
2. **特征工程**：PCA/ICA 即寻找协方差矩阵的特征向量或独立基。
3. **优化算法**：梯度、牛顿法中的 Hessian、二阶近似全部依赖矩阵微积分。
4. **正则化**：$\ell_2$ 正则限制向量的 Euclidean 范数，$\ell_1$ 正则鼓励稀疏系数。
5. **深度学习**：注意力矩阵、卷积核展开、BatchNorm 统计量都可用线性映射描述。

**实例**：
- 数据标准化：对房价特征（面积、房龄、卧室数）做零均值后再训练线性回归，相当于让模型在“去掉公共偏移量”的子空间里拟合。
- PCA：将 784 维的 MNIST 手写图片降到 32 维时，只保留前 32 个最大的奇异值和特征向量，降低存储成本的同时保留主要笔画结构。
- 深度学习：Transformer 中 Query 和 Key 的点积本质是比较两个向量在同一子空间的对齐程度，奇异值过大时常用谱归一化控制注意力权重的放大倍数。

## 13. 建议的学习路线

1. **向量直觉阶段**：手画二维、三维向量的加法、点积、投影，理解长度与角度的含义。
2. **矩阵运算阶段**：练习矩阵与向量、矩阵与矩阵的乘法，关注形状匹配与变换效果。
3. **行列式与消元阶段**：手算 2×2、3×3 行列式，掌握高斯消元和秩的概念。
4. **谱分析阶段**：从对称矩阵入手求特征值/特征向量，并用真实数据做 PCA。
5. **分解与数值阶段**：实现 Gram-Schmidt、QR、SVD，理解数值稳定性和条件数。
6. **应用阶段**：编程实现线性回归、低秩图像压缩、推荐系统嵌入，观察每一步的线性代数含义。

## 14. 练习与资源

- **可视化课程**：3Blue1Brown 的 *Essence of Linear Algebra* 动画。
- **系统教材**：Gilbert Strang 的 *Introduction to Linear Algebra* 及 MIT 公开课 18.06。
- **编程练习**：使用 NumPy/PyTorch 验证矩阵运算、SVD、最小二乘公式。
- **自测题**：自己构造小矩阵，判断其可逆性、特征值，并在坐标纸上描绘变换后的网格。

循序渐进地把抽象概念与“箭头如何移动”“数据方差指向哪里”等可视图像绑定，线性代数就能真正成为解决问题的工具而非考试公式。
